Azure Resource Scope Inheritence Order -

	Management group > Subscription > Resource group > Resource. 
	Scopes are structured in a parent-child relationship. 
	Each level of hierarchy makes the scope more specific.

-------------------------------------------------------------------------------------------------------------

Message Broker - 

	A message broker is a software or service that acts as an intermediary between different applications or components 
	within a distributed system, facilitating the exchange of messages or data between them. Message brokers play a crucial
	role in enabling asynchronous communication and decoupling the producers and consumers of messages, which can enhance
	the scalability, flexibility, and reliability of an application architecture. Here are some key characteristics and 
	functions of a message broker:

	* Message Routing: Message brokers receive messages from producers and route them to one or more consumers based 
	on predefined rules or criteria. This routing can be based on message content, destination, or other attributes.

	* Publish-Subscribe: Message brokers often support a publish-subscribe (pub-sub) model, where producers publish 
	messages to specific topics or channels, and multiple consumers subscribe to those topics to receive relevant messages.

	* Point-to-Point Messaging: In addition to pub-sub, message brokers typically support point-to-point messaging, 
	where a message is sent from one producer to a specific consumer or queue.

	* Message Transformation: Message brokers can perform message transformation, including data format conversion, 
	enrichment, or validation, before delivering messages to consumers.

	* Message Persistence: Messages are often persisted in the message broker to ensure durability and reliability, 
	even if consumers are temporarily unavailable. This can be crucial for preventing message loss.

	* Message Prioritization: Some message brokers allow for the prioritization of messages, ensuring that high-priority 
	messages are processed before lower-priority ones.

	* Dead Letter Queues: Message brokers often include dead letter queues or mechanisms for handling failed messages, 
	allowing developers to investigate and handle problematic messages.

	* Load Balancing: Message brokers can distribute messages evenly among multiple consumers to achieve load balancing 
	and ensure efficient processing.

	* Message Acknowledgment: Consumers typically acknowledge the successful processing of messages to the message broker,
	allowing the broker to remove them from the queue or topic.

	* Security: Message brokers provide security features such as authentication, authorization, and encryption to protect
	the messages being transmitted.

	* Scalability: Message brokers are designed for scalability, allowing them to handle a large volume of messages and adapt 
	to changing workloads.

	* Monitoring and Management: Message brokers often offer monitoring and management tools to track message flow, 
	diagnose issues, and configure broker settings.

	Some popular message brokers and messaging services include Apache Kafka, RabbitMQ, Apache ActiveMQ, AWS SQS (Simple Queue Service),
	Azure Service Bus, and Google Cloud Pub/Sub. The choice of a message broker depends on the specific requirements of your 
	application and the cloud platform or infrastructure you are using.


-------------------------------------------------------------------------------------------------------------
Azure Messaging Services

	Under Azure Messaging Services we have multiple services out of these the most important are:
	1. Azure Service Bus
	2. Event Hub
	3. Event Grid

------------------------------------------------------------
Azure Service Bus 

	It is a fully managed enterprise Message Broker with message queueing and publish subcribe topics.
	It is used to fully DECOUPLE applications and services and provide reliable message queueing and topic subscription models. 
	Messaging capabilities includes :
	* Queues	(1 sender : 1 receiver system)
	* Topic and Subscription	(1 sender : N receiver system) with Rules and actions


	You can use the following protocols with Azure Service Bus to send and receive messages:
		-> Advanced Message Queuing Protocol 1.0 (AMQP)
		-> Hypertext Transfer Protocol 1.1 with TLS (HTTPS)



Service Bus Namespace - Container for all the messaging services - Queues and Topics

NOTE -
	In Azure Service Bus, you can create both queues and topics with the same name within a Service Bus namespace. 
	However, you cannot have a trigger (such as an Azure Function trigger) with the same name as a queue or topic 
	within the same Service Bus namespace. Here's why:

	* Queues and Topics: Queues and topics are the primary messaging entities in Azure Service Bus. 
	They are used for sending and receiving messages. You can create multiple queues and topics within 
	a Service Bus namespace, and each can have a unique name.

	* Triggers: Triggers, on the other hand, are typically associated with serverless compute resources 
	like Azure Functions or Logic Apps. When you create a trigger in Azure Functions, it listens to a specific queue
	or topic for incoming messages. You configure the trigger with the name of the queue or topic it should listen to.

	For example, you can create an Azure Function with a Queue Trigger and specify the name of a queue (e.g., "myqueue")
	that the function should respond to.

	Similarly, you can create an Azure Function with a Service Bus Topic Trigger and specify the name of a topic (e.g., "mytopic") 
	and a subscription within that topic.

	Because triggers are associated with specific messaging entities (queues or topics), it's important to choose unique names 
	for your triggers to avoid naming conflicts with existing queues or topics within the same Service Bus namespace.

	In summary, you can have queues, topics, and triggers within the same Azure Service Bus namespace, but the names 
	of queues/topics and triggers must be distinct to prevent naming conflicts and ensure that messages are routed correctly 
	to the intended triggers.


------------------------------------
 
 Use Case - SERVICE BUS QUEUE 

	 We can create a Queue with data fed for
		* Max Queue size
		* Max delivery count - ??
		* Message time to live

		Click on the Shared Access Policies -> Get the Key

		To code 

		Step 1 : Include Nuget Package
	
			Azure.Messaging.ServiceBus

		Step 2 : Add the code to send the service bus 
	
			[HttpPost]
			public async Task MyServiceMethod(Object myObject)
			{
				var client = new ServiceBusClient(<ConnectionStringOfServiceBus>);
				var sender = client.CreateSender("MY_QUEUE_NAME"); //Depending upon use we can use queue to topic name

				var bodyMsg = JsonConvert.SeriablizeObject(myObject);
				var message = new ServiceBusMessage(bodyMsg);

				//If the msgContain certain keyword then the msg will be push to queue with 15sec delay
				if(bodyMsg.Contains("MY_KEYWORD"))
					message.ScheduledEnqueueTime = DateTimeOffset.UtcNow.AddSeconds(15);

				//If the msg contains TTL then the msg if it stays in queue without getting pulled 
				//by Azure functions more than 20secs. 
				//Then the message will be removed from the queue and deleted (not dead-letter)
				if(bodyMsg.Contains("ttl"))
					message.TimeToLive = TimeSpan.FromSeconds(20);

				await sender.SendMessageAsync(message);		//Also we have SendMessagesAsync for multiple messages
			}

		In the portal we go to Service Bus -> Click on Service Bus Explorer
			Here we would get 3 options
			- Send : To Resend the data
			- Receive : To check + delete the content
			- Peek :  To check message from Queue + Deadletter

		Note - DeadLetter is a sub-queue to keep data which were not processed from the queue

		-------------------------------------------------

		Step 3 : 
		Use the queue message - We use Azure function. To run Azure Functions locally we need Azure Core Tools

		To use Azure Function let us use .net code functions
		1. Azure web job storage - Select as Storage Emulator
	
		Azure Function Code

		public static class MyFunction
		{
			[FunctionName("MyFunction")]
			public static void Run
			(	[ServiceBusTrigger("MY_QUEUE_NAME", Connection = "ConnString")] string myQueueItem
				, ILogger log)
				{
					//Here if we get the exception we throw - Now the msg will remain in the queue with DELIVERY COUNT = DELIVERY COUNT + 1
					//Once the DELIVERY COUNT reaches the max delivery count limit of the queue then it will be pushed to DEAD LETTER queue 
					//Ex : If queue has delivery count = 10, our msg once completes 10 iteration from Queue <-> functions will be pushed to 
					//Dead Letter with delivery count = 11

					if(myQueueItem.Contains("KEYWORD"))
						throw new Exception();

					//DO SOME WORK
				}
			}
		}

		Here 
		myQueueItem - object that holds the data JSON serialized to string data

		----------------------------------------
		Service Bus Queue Properties 

		1. Service Bus Queues can be of Sizes between 1GB and 80GB
		2. Max message size between 256KB to 100MB
		3. TTL = TimeSpan.MaxValue
		4. Max No. Of Queues = 10000 (per service namespace)
		5. Max No. of Concurrent Clients = 5000

		Service Bus Queue tree -> ServiceBus NameSpace/ServiceBus Queue

		- Azure Service Bus queue message removed after TTL then it will be deleted 
		- We cannot have Queue and Topic with same name in a Service Bus namespace.

------------------------------------------------------------------------------------------------------------------------------

Use Case - SERVICE BUS TOPICS

	Concept -> Sender -> Service Bus Topic -> N Subscriber (Pub-Sub Model)
		Service Bus Topic tree -> ServiceBus NameSpace/ServiceBusTopic/TopicSubscriber

	To code 

		Step 1 : Create a Topic
			This TOPIC will store the message sent from Code. And later will be consumed by a SUBSCRIBER

		Step 2 : Include Nuget Package
	
			Azure.Messaging.ServiceBus

		Step 3 : 

		[HttpPost]
		public async Task Post(WeatherForecast obj)
		{
			var client = new ServiceBusClient("CONNECTION_STRING");
			var sender = client.CreateSender("MY_BUS");

			var msgBody = JsonConvert.SerializeObject(obj);
			var sbMessage = new ServiceBusMessage(msgBody);
	
			await sender.SendMessageAsync(sbMessage);	//Also we have SendMessagesAsync for multiple messages
		}

		Step 4 : To receive the message we need to create a SUBSCRIPTION
		In the portal -> Topic -> LHS Menu click on SUBSCRIPTION

		Add Name, Max Delivery Count, Auto Delete after idle for, Time to live, Message lock duration

		Note: For a Topic we can have many Subscriber. Any Subscriber created now will not be able to 
		access the content previous data of topic.

		Step 5 : Create a Azure Function
		We would create a Service Bus Topic Trigger

		//Subscription 1 is used by "MyFunction"
		[FunctionName("MyFunction"]
		public void Run
			([ServiceBusTrigger("MY_TOPIC", "SUBSCRIPTION1", Connection="MyConnection"), Disable("MY_DISABLE_KEY")] string mySbMsg, ILogger logger)
		{
			//For any exception the delivery count concept will be same as the
			//Queue but the delivery count will be impacted at the subscription
			//And the message will be sent to DEAD LETTER of the subscription
				
			//Do some work
		}

		//Here "MY_DISABLE_KEY" can be implemented in almost all type of functions once if it is true then though triggering element is present 
		//FunctionApp wont be triggered

		//Subscription 2 is used by "SendEmail"
		[FunctionName("SendEmail"]
		public void Run
			([ServiceBusTrigger("MY_TOPIC", "SUBSCRIPTION2", Connection="MyConnection")] string mySbMsg, ILogger logger)
		{
			//Send Email
				
			//Do some work
		}


	----------------------------------------------------------------------------
	
	For subscriptions we can have filters -
		a. SQL Filter
		b. Correlational Filter

	For a subscription we can create a Filter (Correlation)
		Step -> Go to Subscription + click on OVERVIEW -> At bottom we can find Filter 
		Add a filter - Here we are using SQL filter. Let us have the filter logic from portal
	
		Month = 'January'
	
		or we can write like

		Month LIKE 'Jan%'

		Let this filter be added to "Subscription2" for "SendEmail"

	We also have SQL filters

	----------------------------------------------------------------------------

	From the service the data we are pushing we add a 
	Property -> Month with data
	We send the data to TOPIC

	As we have added the filter to "Subscription2" so "SendEmail" will be invoked only in the message the Month Property has value 
	"January"

	The other functions with no Filter will be invoked as Topic receives message.

	Filters can be SQL filter, Boolean Filter, Correlational Filter. We would discuss about Sql filters

	----------------------------------------------------------------------------

	From the code also we can set the Filters.

	[HttpPost]
	public async Task SetRule()
	{
		var client = new ServiceBusClient("CONNECTION_STRING");

		// Create a SQL filter with color set to red
		// Action is defined to set the quantity to half if the color is red
		await client.CreateRuleAsync("MY_TOPIC", "MY_SUBSCRIPTION", 
				new CreateRuleOptions 
				{ 
					Name = "RedOrdersWithAction",
					Filter = new SqlRuleFilter("user.color='red'"),
					Action = new SqlRuleAction("SET quantity = quantity / 2;")
				}
	}

	This will create a rule. Filters with Actions is called Rules.
	Here for Topic = My_Topic
	Subscription = My_Subscription
	We created a rule with Filter if the data passed to topic has color = red then "My_Subcription" will be invoked and 
	Quantity will be divided by 2. 



	Service Bus Properties 
	* Number of subscriptions per topic	Entity	2,000 per-topic for the Standard tier and Premium tier.
	* Number of SQL filters per topic	Entity	2,000
	* Number of correlation filters per topic	Entity	100,000

	Rest of the conditions are same as below (previosly mentioned)
		1. Service Bus Queues can be of Sizes between 1GB and 80GB
		2. Max message size between 256KB to 100MB
		3. TTL = TimeSpan.MaxValue
		4. Max No. Of Queues = 10000 (per service namespace)
		5. Max No. of Concurrent Clients = 5000

--------------------------------------------------------------

Azure Messaging Services Comparison

	AZURE EVENT GRID
		- Serverless event bus for Azure service to service communication
		- Atleast once delivery of message
		- Low cost

	AZURE EVENT HUB (typically will be used for Big Data)
		- Streaming data service
		- Low latency
		- Can receive and process millions of events per seconds
		- Comparatively expensive

	AZURE SERVICE BUS
		- Queue or Pub/Sub for web application
		- Polling or checking at regular interval


----------------------------------------------------------

Azure event Grid

	It is used for Serverless events. Azure Event Grid is a highly scalable, fully managed Pub Sub message distribution service 
	that offers flexible message consumption patterns using the MQTT and HTTP protocols. 

	Event grid can accept any information from any of the Azure Services and respond back to any other azure services.
	The purpose of the event grid is to receive an event(event grid message) and relay to downstram service to perform certain actions.



---------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

Use Case - AZURE STORAGE SERVICES 

This will have different storage mechanism -
	
	1. Azure blob storage 
		a. Also Called Containers. 
		b. It is a scalable object store for text and binary data (for unstructured data). (190TB)
		c. Data can be accessed using SAS token. This is a secured access.
		d. Can store large amount of unstructed data files.
		e. Can store any kind of files.

	2. Azure files storage 
		a. Used when we want to share file between multiple VMs or Kubernetes pods. 
		That is if multiple VMs want to access common files.
		b. As it can be shared amongst multiple VMs there is a backup policy associated with the file with a backup frequency.
		c. It basically provides the file explorer structure for the VMs.
		d. Default capacity is 5TB.
		e. Azure Files offers two industry-standard protocols for mounting Azure file share: 
		the Server Message Block (SMB) protocol and the Network File System (NFS) protocol. 
		Azure Files enables you to pick the file system protocol that is the best fit for your workload. 
		Azure file shares don't support accessing an individual Azure file share with both the SMB and NFS protocols, 
		although you can create SMB and NFS file shares within the same storage account. For all file shares, Azure 
		Files offers enterprise-grade file shares that can scale up to meet your storage needs and can be accessed 
		concurrently by thousands of clients.
			i. SMB File Share
			
			SMB file shares are used for a variety of applications including end-user file shares and file shares 
			that back databases and applications. SMB file shares are often used in the following scenarios:
				- End-user file shares such as team shares, home directories, etc.
				- Backing storage for Windows-based applications, such as SQL Server databases or 
				line-of-business applications written for Win32 or .NET local file system APIs.
				- New application and service development, particularly if that application or service 
				has a requirement for random IO and hierarchical storage.

			ii. NFS File Share

			NFS file shares are often used in the following scenarios:
				- Backing storage for Linux/UNIX-based applications, such as line-of-business applications 
				written using Linux or POSIX file system APIs (even if they don't require POSIX-compliance).
				- Workloads that require POSIX-compliant file shares, case sensitivity, or Unix style permissions (UID/GID).
				- New application and service development, particularly if that application or service has a requirement 
				for random I/O and hierarchical storage.

			Security - All data stored in Azure Files is encrypted at rest using Azure storage service encryption (SSE). 
			Storage service encryption works similarly to BitLocker on Windows: data is encrypted beneath the file 
			system level. Because data is encrypted beneath the Azure file share's file system, as it's encoded to
			disk, you don't have to have access to the underlying key on the client to read or write to the Azure file 
			share. Encryption at rest applies to both the SMB and NFS protocols.

			By default, all Azure storage accounts have encryption in transit enabled.
		f. To share files amongst multiple VMs -> Click on the "Connect" button at the top. Select the OS -> Windows/Linux/MacOS.
		The Click on the show script. and then we need to execute on the terminal. Ex - for windows we can execute the script on the
		Powershell terminal.

	3. Azure tables (no sql) 
		a. NoSQL store for non-relation data. That is storing semi-structed data.

	4. Azure queues
		a. Messaging Store for messaging between applications

	Blobs storage advantage - Scalable: Blob storage capacity is practically unlimited. 
	And as the amount of stored data grows, it remains easy and fast to save data for later retrieval.

	-----------------------------------------------------------------------------------------------------------------

	Blob Access Tier 
	
	Azure Blob Storage is a cloud-based object storage service provided by Microsoft Azure. It is designed to store and manage 
	unstructured data, such as documents, images, videos, backups, and logs. Azure Blob Storage offers three different access 
	tiers to optimize the cost and performance of storing your data, each tailored for specific use cases:

	Hot Access Tier:

	The Hot Access Tier is designed for data that is frequently accessed and requires low-latency access times.
	It offers the highest storage costs but provides the fastest data retrieval performance.
	Use cases include frequently accessed data, serving content to users, and data that needs to be readily available for immediate access.

	Cool Access Tier:

	The Cool Access Tier is optimized for data that is less frequently accessed but still requires relatively fast access times.
	It offers lower storage costs compared to the Hot tier but with slightly higher data retrieval costs.
	Use cases include data archiving, backup, and infrequently accessed logs.

	Archive Access Tier:

	The Archive Access Tier is the most cost-effective option for data that is rarely accessed and can tolerate longer retrieval times.
	It has the lowest storage costs but higher data retrieval costs and longer retrieval times (retrieval times can take hours).
	Use cases include long-term archival storage, compliance, and data that is rarely accessed but needs to be retained.
	When choosing the appropriate access tier for your data in Azure Blob Storage, consider factors such as data access patterns, 
	access frequency, and the cost sensitivity of your application. Here's a general guideline:

	Use the Hot Access Tier for data that requires fast and frequent access.
	Use the Cool Access Tier for data that is accessed less frequently but still needs relatively quick retrieval times.
	Use the Archive Access Tier for data that is rarely accessed and can tolerate longer retrieval times, with a primary 
	focus on cost optimization.
	Azure Blob Storage allows you to transition data between these access tiers as your requirements change. 
	This tiering capability enables you to optimize storage costs while ensuring that your data remains accessible when needed. 
	Keep in mind that transitioning data between tiers may involve certain rules and costs, so it's essential to plan your data 
	storage strategy carefully.

	-----------------------------------------------------------------------------------------------------------------

	Blob Lease - It is a mechanism to acquire lock on Blob for a duration of between 15 and 60 secs 
	(there are cases where it can be extended for further duration).
	Basically if the blob storage is used by two clients say client 1 and client 2. So here if client 1 is modifying client 2 should not be
	allowed to to modify (its kind of lock mechanism).So here client 1 will acquire the lease (acquire lease) for updation once updation is done it will
	release the lease (delete lease). Then client 2 can work on it.
	
	NOTE - If we have a blob lease activated then it will not allow us to modify the blob file programitically.
	To work from code in the .net, we need:

	//Program to use a lease created on blob to update a file. Till the program releases the lease no other resource can modify it.
	using Azure.Storage.Blobs;
	using System.IO;

	public class BlobLease{
		
		string const connectionStr = "<CONNECTION_STR>"
		string const containerName = "<CONTAINER_NAME>"
		string const blobFileName = "<BLOB_FILE_NAME>"
		string const blobFileLeaseId = "<LEASE_ID>"

		public void FunctionBlobLease(){
			BlobContainerClient containerClient = new BlobContainerClient(connectionString, containerName);
			BlobClient blobClient = containerClient.GetBlobClient(blobFileName);
			
			MemoryStream stream = new MemoryStream();
			blobClient.DownloadTo(Stream);

			StreamWriter writer = new StreamWriter(stream);
			writer.WriteLine("Data for lease demo");
			writer.flush();

			stream.Position = 0;

			BlobUploadOptions uploadOptions = new BlobUploadOptions
			{
				Conditions = new BlobRequestConditions
				{
					LeaseId = blobFileLeaseId
				}
			};

			//blobClient.Upload(stream, true);	//Normal Upload without lease
			blobClient.Upload(stream, uploadOptions);	//Lease Upload

			//Releasing the lease on the blob
			BlobLeaseClient leaseClient = new BlobLeaseClient(blobClient, blobFileLeaseId);
			leaseClient.Release();
		}
	}
	
	//Program to fetch and release a lease on the blob to update a file. Till the program releases the lease no other resource can modify it.
	using Azure.Storage.Blobs;
	using System.IO;

	public class BlobLease{
		
		string const connectionStr = "<CONNECTION_STR>"
		string const containerName = "<CONTAINER_NAME>"
		string const blobFileName = "<BLOB_FILE_NAME>"

		public void FunctionBlobLease(){
			BlobContainerClient containerClient = new BlobContainerClient(connectionString, containerName);
			BlobClient blobClient = containerClient.GetBlobClient(blobFileName);
			BlobLeaseClient leaseClient = new BlobLeaseClient(blobClient, blobFileLeaseId);
			string leaseId = leaseClient.Acquire(TimeSpan.FromSeconds(60)).Value.LeaseId;	//Lease Id creation with 60 sec window

			MemoryStream stream = new MemoryStream();
			blobClient.DownloadTo(Stream);

			StreamWriter writer = new StreamWriter(stream);
			writer.WriteLine("Fetched Lease Id Data for lease demo");
			writer.flush();

			stream.Position = 0;

			BlobUploadOptions uploadOptions = new BlobUploadOptions
			{
				Conditions = new BlobRequestConditions
				{
					LeaseId = leaseId
				}
			};

			//blobClient.Upload(stream, true);	//Normal Upload without lease
			blobClient.Upload(stream, uploadOptions);	//Lease Upload

			leaseClient.Release();
		}
	}
	
--------------------------------------------------------------------------------------------------

Service Bus Queue vs Storage Bus Queue. When to use what?
	
	Azure Storage Queues:
		Purpose: Designed for high-throughput scenarios that require a large volume of messages.
		Features:
			a. Simple: Provides a uniform and consistent programming model across queues, tables, and BLOBs.
			b. Message Size: Supports messages up to 64 KB in size.
			c. Storage Capacity: Can store millions of messages, up to the total capacity limit of a storage account.
			d. Progress Tracking: Useful for tracking message processing progress, even if a worker crashes.
			e. Server-Side Logs: Logs all transactions executed against your queues.
		Consider Using Storage Queues When:
			a. Storing over 80 gigabytes of messages in a queue.
			b. Tracking message processing progress.
			c. Needing server-side logs of queue transactions.

	Azure Service Bus Queues:
		Purpose: Ideal for high-value enterprise messaging scenarios.
		Features:
			a. Advanced: Offers more complex messaging capabilities.
			b. Receive Without Polling: Allows receiving messages without polling the queue (long-polling receive operation).
			c. Guaranteed FIFO Order: Provides guaranteed first-in-first-out (FIFO) ordered delivery.
			d. Duplicate Detection: Supports automatic duplicate detection.
			e. Parallel Long-Running Streams: Messages can be associated with a stream using the session ID property.
			f. Local Transactions: Supports local transactions within a single queue.
		Consider Using Service Bus Queues When:
			a. Receiving messages without polling.
			b. Needing guaranteed FIFO order.
			c. Requiring automatic duplicate detection.
			d. Processing messages as parallel long-running streams.



Storage Table vs Cosmos DB. When to use what?

	Azure Table Storage and Azure Cosmos DB for Table share a table data model and perform similar operations. However, Cosmos DB 
	offers higher performance, global distribution, and automatic secondary indexes. 
	Azure Cosmos DB for Table uses a reserved capacity model in order to ensure guaranteed performance but this means that one pays
	for the capacity as soon as the table is created, even if the capacity isn't being used. With Azure Table storage one only pays 
	for capacity that's used.

	Here's how they compare: 

		Indexing
			Cosmos DB automatically indexes all properties, while Table Storage only supports PartitionKey-RowKey indexing.
			This makes Cosmos DB better for complex queries. 

		Latency
			Cosmos DB promises read latency below 10 milliseconds and write latency below 15 milliseconds. Table Storage doesn't make any latency promises. 
	
		Availability
			Cosmos DB is designed for geographic availability. You can create writable nodes globally, making your application accessible everywhere. 
	
		Payment
			Cosmos DB uses a reserved capacity model, so you pay for capacity as soon as the table is created, even if it's not being used. Table
			Storage charges only for used capacity. 
	
		Use cases
			Cosmos DB is suitable for situations requiring high throughput, minimal latency, and control over failover scenarios. It's also a good 
			choice for real-time IoT and telemetry workloads, real-time order processing, and supply chain applications. Table Storage is suitable
			 for flexible datasets like address books, user data for web apps, and device information. 


--------------------------------------------------------------------------------------------------

Storage Browser can be used to check all the storage components at a glance.

--------------------------------------------------------------------------------------------------

Storage account also have the option to host STATIC WEBSITE
	1. Under data management tab we have the option Static website.
	2. Click on "Enabled" on the opened modal page.
	3. Give the "Index document path" and "Error document path".
	4. Save the context. A container and a webaddress will be created. 
	5. Go to the container and add the file as provided in the path of Index Document and Error Document.

--------------------------------------------------------------------------------------------------

Azure Storage Redundancy Options - Mode for Disaster Recovery.

	Redundancy Options - 
	1. Redundancy In Primary Region
		
		a. Locally Redundant Storage (LRS) - Recommended for NON-CRITICAL scenarios.
			- Simple amongst all the options. Most economical option.
			- Redundancy in a single region.
			- Data replicated three times in the same data center. 
			- Providing SLA of atleaast 11 9s of durability. (Availability of 99.999999999%)
			Limitation - Data loss incase of local disaster.

		b. Zone Redundant Storage (ZRS) - Recommended for BACKUP scenarios.
			- Replicate the data amongst three azure availability zones (paired regions). Data available even if one zone is down.
			- Providing SLA of atleaast 12 9s of durability. (Availability of 99.9999999999%)
			Use Case - High availability.

	2. Redundancy In Secondary Region (SLA of 16 9s)
		
		a. Geo-Redundant Storage (GRS) - Recommended for HIGH-AVAILABILITY scenarios.
			- Replicate data synchronously in the primary region and asynchronously in the secondary region. 
			So the data of the primary region will be copied to the secondary region.
			- Both the region will have three copies.
			- But data in secondary region is available only after failover or read access in enabled.

		b. Geo-Zone Redundant Storage (GZRS) - Recommended for CRITICAL DATA scenarios.
			- There will be zone redundancy in the primary region and a secondary zone. 
			That is three copies will be synchronously spread across three availability primary zone. 
			And then the three copies from primary zone will be replicated the secondary zone.
			- Ideal choice for maximum consistancy, durability, availability and resilience. 
			- Provide read access to data in the secondary region.			 

	2a. Read Access In Secondary Region (SLA of 16 9s)
		a. Read-Access Geo-Redundant Storage (RA-GRS) - 
			- Same as GRS with read access to secondary region.
		b. Read-Access Geo-Zone Redundant Storage (RA-GZRS) - 
			- Same as GZRS with read access to secondary region.

	NOTE - More the redundancy more the cost.

--------------------------------------------------------------------------------------------------

Azure availability zones, Availability set, region  - 

	Storage Hierarchy 

	Geography -> This represents a particular geographical seperated areas.
		Regions -> Each geography can have multiple regions.
			Availability Zones -> Regions may have one or more availability zones. A region will have upto three availability zone.
				Data Centers -> Availability zones are collection of Data Centers.
					Fault Domain -> Within data centers we have server racks. Each of the server racks is called a fault domain.
						Update Domain -> Each server rack will have multiple servers. These servers will be called an update domain.

	Availability zone redundancy 
		- Multiple instances are stored amongst different availability zone.
		- Protects against fault at data center level.
	Availability Set redundancy 
		- Here instances are stored as copies amongst multiple "Fault Domain" (i.e multiple server racks in a data center).
		- Protects against fault at the server rack level.		

	Use Case -
	1. Single VM 
		- SLA 99.9% with premium storage.
		
	2. Two VM
		- Redundancy with two availability sets.
		- SLA 99.95% 

	3. Two VM
		- Redundancy with two availability zones.
		- SLA 99.99%

	4. To protect machines against disaster recovery
		- Need to place across multiple regions

--------------------------------------------------------------------------------------------------


Stored Access Policy Vs Shared Access Signature
	
	Stored Access Policy is the policy of the Azure Storage account which determine how access to storage account resources
	will be configured.

	Shared access signature token (SAS Token) which provides a temporary token accessilble for limited time which can be shared
	to access a particular resource.


---------------------------------------------------------------------------------

Use Case - AZURE QUEUE STORAGE

This is a queueing solution for handling "large volume" of data. It allows asynchoronous communication between applications
This allows to decouple components, build resilience and also scale traffic.

It is a service to store large volume of data.

	A queue message can be upto 64KB in size.

	Typically the address format is https://<MY_AZURE_STORAGE_ACCOUNT>.queue.core.windows.net/<MY_QUEUE>



Let us implement

	Step 1 : Now create a Azure Storage Account + Azure Storage Queue
	
	When we push a message to queue it will have insertion time, deletion time (by default it will be 7 days).

	Step 2 : Use nuget package
		Azure.Storage.Queues

	Step 3 : Create a service which deals with long running task. So we will use backgraound process to run them.

	[HttpPost]
	public async Task Post([FromBody] Weatherforecast data)
	{
		var queueName = "MY_STORAGE_QUEUE";
		var queueClient = new QueueClient("CONN_STR", queueName);

		var message = JsonConvert.SerializeObject(data);

		//Visibility timeout - After how much time after push the messasge appears in the queue
		var visibilityTimeOut = TimeSpan.FromSeconds(10);	
		
		//Expiration time
		//Here as visibility timeout is 10secs and expiration timeout if it was 10secs then
		//the message will never appear in the queue. So we must have higher value. Now it is 20secs
		//As such the message stays in the queue for 20-10 = 10secs
		//Now if we want the message to never expire we can set the value TimeSpan.FromSeconds(-1);
		var expirationTime = TimeSpan.FromSeconds(-1);		
		
		await queueClient.SendMessageAsync(message, visibilityTimeOut, expirationTime);		//Also we have SendMessagesAsync for multiple messages
	}

	Step 4 : Create a task for with backgound running task.

	public class WeatherDataService : BackgroundService
	{
		private readonly ILogger<WeatherDataService> _logger;
		public WeatherDataService(ILogger<WeatherDataService> logger)
		{
			_logger = logger;
		}

		protected override async Task ExecuteAsync(CancellationToken stoppingToken)
		{
			var queueName = "MY_STORAGE_QUEUE";
			var queueClient = new QueueClient("CONN_STR", queueName);

			//Till the application keeps running IsCancellationRequested will be false
			while(!stoppingToken.IsCancellationRequested)
			{
				//When we read the message from the queue it will be temporarily disappear from the queue for certain period.
				//The default visibiltiyTimeout for the ReceiveMessageAsync() is 30 secs. That is when read this message will
				//be hidden from the queue for 30secs.
				var queueMsg = await queueClient.ReceiveMessageAsync();	//Also we have ReceiveMessagesAsync

				if(queueMsg.Value != null)
				{	
					//Data received in Json Format
					var weatherData = JsonConvert.DeserializeObject<Weatherforecast>(queueMsg.Value.MessageText);

					//Do some task

					//This deletion is essential else message will not be deleted from the queue
					//If it is a long running process or thrown an exception, as such the delete command is not hit
					//so the message will appear in the Queue for other application to process.
					await queueClient.DeleteMessageAsync(queueMsg.Value.MessageId, queueMsg.Value.PopRecipt);
				}

				//This becomes a background service will polls queue message every 10 secs
				await Task.Delay(TimeSpan.FromSeconds(10));
			}
		
		}
	}
	
	In the Startup.cs we need to add references

	ConfigureServices(IServiceCollection services)
	{
		services.AddHostedService<WeatherDataService>();
	}

------------------------------------------------------------------------

We can also use it as Queue Trigger
//Please note the function name must be unique for a function namespace
	
	[FunctionName("ProcessWeatherData")]
	public static void Run
	([QueueTrigger("MY_QUEUE", Connection = "CONN_STR")] string myQueueItem, ILogger logger)
	{
		if(myQueueItem.Contains("EXCEPTION_STR")) throw new Exception();

		//Do some work
	}
	
	Here let the string has the content "EXCEPTION_STR" now our message is not getting processed, as such in the queue
	the DEQUEUE Count will increase. Once the queue content reaches threshold DEQUEUE COUNT this content will be pushed 
	to a new queue MY_QUEUE-poison, here "-poison" will be added to our queue name and will be send in such scenario.
	Please note for now there is no poison queue so the message will not be accessible.

	As for previous scenario poison queue is now created but the last message is not accessible. Now once created a similar
	error condition will throw the message in the newly created poison queue and we will be able to access it.

	For the Dequeue count it will be say at initiation 5 so in the poison queue it will be 0 (5-1-1-1-1-1 -> 0)

	While reading the message from functions to Azure Queue Storage we can configure the max dequeue count in the 
	host.json file - this file has all the configurations related to metadata of the functions 
	
	
---------------------------------------------------------------------------------------------------------------------------------



Azure Queue Storage (small msg size + larger volume) vs Azure Service Bus Queue (big msg size + small volume)

	-> Queue Storage - A simple message queue service to store large number of messages.
	 SB Queue - Broader messaging service that supports queing, pub-sub approach.

	-> QS 
		- Can handle million of messages, no order gaurantee.
		- Atleast once delivery
		- 500TB max queue size
		- 64KB message size
		- Unlimited queues
		- Unlimited concurrent clients
		- Leased based access mode 30 sec to 7 days
	-> SBQ 
		- Offers FIFO ordering
		- Atleast Once or Atmost once delivery
		- 1GB to 80GB max queue size
		- 256KB to 1MB message size
		- 10000 max queues
		- 5000 concurrent clients
		- Lock based access mode (by default 60 secs)

---------------------------------------------------------------------------------------------------------------------------------

We are going to implement AZURE BLOB STORAGE CONTAINER BASED EVENT GRID TRIGGER


	Let us create a Storage account -> Select Blob or Container.
	Now let us create a Event Grid Trigger for Azure Functions in the same Storage account

	[FunctionName("EventGridTriggerDemo")]
        public static void Run([EventGridTrigger] EventGridEvent eventGridEvent, ILogger log)
        {
            log.LogInformation(eventGridEvent.Data.ToString());
        }

-----------------------------------------------------------------------------------

Azure SQL server -

1. Managed  by cloud
2. Pay as you use
3. Higher availability
4. PAAS offering




































